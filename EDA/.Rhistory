# Plotting the missing data pattern
vis_miss(Data)
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, results='asis', warning=FALSE, message=FALSE, fig.width=5, fig.height=4, fig.align='center', error = FALSE)
library(readr)#Loading data into R
library(tidyverse)#Data manipulation
library(flextable)#Table output
library(VIM)#Visualizing missing values
library(naniar)#For handling missing data
library(kableExtra)#Table
library(ggplot2)#Data visualization
Data = read_csv("train.csv")
#Checking for duplicates in the key variable
anyDuplicated(Data$Id)
#Is there any missing values
any(is.na(Data))
#Proportion of missing values per variable
aggr(Data[, colSums(is.na(Data))>0], plot = T)
#80% of the rows
miss = 0.8 * length(Data$Id)
#Eliminating variables with more than 80% missing variables
Data = Data[, !colSums(is.na(Data))>miss]
#Sum of missing values per variable
colSums(is.na(Data))
# Plotting the missing data pattern
vis_miss(Data)
# Applying Little's MCAR test
mcar_test(data)
# Applying Little's MCAR test
mcar_test(Data)
?mcar_test
head(Data)
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
mcar_test(data.frame(Data))
#Sum of missing values per variable
colSums(is.na(Data))
summary(Data)
#Sum of missing values per variable
colSums(is.na(Data))>0
#Sum of missing values per variable
colSums(is.na(Data))
#Sum of missing values per variable
colSums(is.na(Data))[colSums(is.na(Data))>0]
summary(Data)
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>%  select(where(is.numeric)) %>% colnames()
Num
mcar_test(data.frame(Data[Num]))#numerical variables
summary(Data[Num])
colSums(is.na(airquality))
mcar_test(airquality)
mcar_test(Data[Num])#numerical variables
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>%  select(where(is.numeric) $ where(is.na)) %>% colnames()
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>%  select(where(is.numeric $ is.na)) %>% colnames()
is.na(Data[Num])
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>%  select(where(is.numeric $ colSums(is.na(Data))>0)) %>%
colnames()
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>%  select(where(is.numeric & colSums(is.na(Data))>0)) %>%
colnames()
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>%  select(where(is.numeric & is.na(Data))) %>%
colnames()
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>%  select(where(is.numeric)) %>% select(where(is.na(Data)))
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>%  select(where(is.numeric)) %>% select(where(is.na))
is.numeric(Data)
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>% select(where(is.na))
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>% select(where(is.na)) %>%
colnames()
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>% select(where(is.numeric)) %>%
colnames()
install.packages("mice")
library(mice) # Missing value imputation using chained equation
?mice
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, results='asis', warning=FALSE, message=FALSE, fig.width=5, fig.height=4, fig.align='center', error = FALSE)
library(readr)#Loading data into R
library(tidyverse)#Data manipulation
library(flextable)#Table output
library(VIM)#Visualizing missing values
library(naniar)#For handling missing data
library(mice) # Multiple imputation using chained equation
library(kableExtra)#Table
library(ggplot2)#Data visualization
Data = read_csv("train.csv")
Data
Data = read.csv("train.csv", stringsAsFactors = T)
rm(Data)
rm(Data)
Data = read.csv("train.csv", stringsAsFactors = T)
head(Data)
summmry(data)
summmry(Data)
summary(Data)
#Checking for duplicates in the key variable
anyDuplicated(Data$Id)
#Is there any missing values
any(is.na(Data))
#Proportion of missing values per variable
aggr(Data[, colSums(is.na(Data))>0], plot = T)
#80% of the rows
miss = 0.8 * length(Data$Id)
#Eliminating variables with more than 80% missing variables
Data = Data[, !colSums(is.na(Data))>miss]
#Sum of missing values per variable
colSums(is.na(Data))[colSums(is.na(Data))>0]
# Plotting the missing data pattern
vis_miss(Data)
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>% select(where(is.numeric)) %>%
colnames()
mcar_test(Data[Num])#numerical variables
Num
# Outliers in numerical variables
Outliers = lapply(Data[Num], function(x) boxplot.stats(x)$out)
Outliers
# Outliers in each numerical variables
Outliers = sapply(Data[Num], function(x) boxplot.stats(x)$out)
Outliers
summary(Data)
c(Data[2])
Data[["id"]]
Data[,"id"]
names(Data)[1]
Data[["Id"]]
Data[,"Id"]
Data[,1]
# Eliminating outliers
Data[!Data[,2] %in% Outliers[2],]
!Data[,2] %in% Outliers[2]
!Data$MSSubClass %in% Outliers[2]
!Data$MSSubClass %in% Outliers$MSSubClass
intersect(Data[,2],Outliers[2])
a=c(1,2,3,3,4,5,6)
b=c(1,3,5,7,9)
intersect(a,b)
union(a,b)
diff(a,b)
diff(b,a)
?intersection
setdiff(a,b)
setdiff(Data[,2],Outliers[2])
Data[,2]
Outliers[2]
# Identify outliers using IQR method
Q1 <- quantile(Data[,2], 0.25)
Q1
Q3 <- quantile(Data[,2], 0.75)
IQR <- Q3 - Q1
IQR
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, results='asis', warning=FALSE, message=FALSE, fig.width=5, fig.height=4, fig.align='center', error = FALSE)
library(readr)#Loading data into R
library(summarytools)#summary of the data
library(tidyverse)#Data manipulation
library(flextable)#Table output
library(VIM)#Visualizing missing values
library(naniar)#For handling missing data
library(mice)#Multiple imputation using chained equation
library(kableExtra)#Table
library(ggplot2)#Data visualization
descr(Data[,2])
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
lower_bound
upper_bound
Outliers[2]
Data[,2]>145
Outliers[2]
Data[Data[,2]>145,2]
Num
# Eliminating outliers
for (i in Num) {
print(i)
}
Data[which(Data[,1] > lower_bound & Data[,1] < upper_bound),]
Data[which(Data[,2] > lower_bound & Data[,2] < upper_bound),]
# Eliminating outliers
for (i in Num) {
# Identify outliers using IQR method
Q1 = quantile(Data[,i], 0.25)
Q3 = quantile(Data[,i], 0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
#filtering the dataset
Data = Data[which(Data[,i] > lower_bound & Data[,i] < upper_bound),]
}
#omitting all rows with missing values
Data = na.omit(Data)
# Eliminating outliers
for (i in Num) {
# Identify outliers using IQR method
Q1 = quantile(Data[,i], 0.25)
Q3 = quantile(Data[,i], 0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
#filtering the dataset
Data = Data[which(Data[,i] > lower_bound & Data[,i] < upper_bound),]
}
Data
Data = read.csv("train.csv", stringsAsFactors = T)
# First six rows in the data frame
head(Data)
# summsry of the data
dfSummary(Data)
# summsry of the data
dfSummary(Data, graph.col = F)
# Checking for duplicates in the key variable
anyDuplicated(Data$Id)
#Is there any missing values
any(is.na(Data))
#Proportion of missing values per variable
aggr(Data[, colSums(is.na(Data))>0], plot = T)
#80% of the rows
miss = 0.8 * nrow(Data)
#Eliminating variables with more than 80% missing variables
Data = Data[, !colSums(is.na(Data))>miss]
#Sum of missing values per variable
colSums(is.na(Data))[colSums(is.na(Data))>0]
#omitting all rows with missing values
Data = na.omit(Data)
any_na(Data)
library(car)
i
Data[,i]
Outliers[i]
# Identify outliers using IQR method
Q1 = quantile(Data_miss[,i], 0.25)
#omitting all rows with missing values
Data_miss = na.omit(Data)
# Identify outliers using IQR method
Q1 = quantile(Data_miss[,i], 0.25)
Q3 = quantile(Data_miss[,i], 0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
upper_bound
lower_bound
Data_miss[Data_miss[i>upper_bound,],i]
Data_miss[Data_miss[i,]>upper_bound,i]
Data_miss[,i]>upper_bound
Data_miss[Data_miss[i]>upper_bound,i]
Data_miss[Data_miss[i]>upper_bound,]
Data_miss[Data_miss[i]<upper_bound,]
which(Data_miss[,i] > lower_bound &
Data_miss[,i] < upper_bound)
Data_miss[Data_miss[i]>upper_bound,i]
Data_miss[Data_miss[i]>upper_bound,]
which(Data_miss[,i] > lower_bound | Data_miss[,i] < upper_bound)
nrow(Data_miss)
Data = read.csv("train.csv", stringsAsFactors = T)
# First six rows in the data frame
head(Data)
# summsry of the data
dfSummary(Data, graph.col = F)
# Checking for duplicates in the key variable
anyDuplicated(Data$Id)
#Is there any missing values
any(is.na(Data))
#Proportion of missing values per variable
aggr(Data[, colSums(is.na(Data))>0], plot = T)
#80% of the rows
miss = 0.8 * nrow(Data)
#Eliminating variables with more than 80% missing variables
Data = Data[, !colSums(is.na(Data))>miss]
#Sum of missing values per variable
colSums(is.na(Data))[colSums(is.na(Data))>0]
nrow(Data_miss)
Data = read.csv("train.csv", stringsAsFactors = T)
# First six rows in the data frame
head(Data)
# summsry of the data
dfSummary(Data, graph.col = F)
nrow(Data)
#Is there any missing values
any(is.na(Data))
#Proportion of missing values per variable
aggr(Data[, colSums(is.na(Data))>0], plot = T)
#80% of the rows
miss = 0.8 * nrow(Data)
#Eliminating variables with more than 80% missing variables
Data = Data[, !colSums(is.na(Data))>miss]
#Sum of missing values per variable
colSums(is.na(Data))[colSums(is.na(Data))>0]
nrow(Data)
#omitting all rows with missing values
Data_miss = na.omit(Data)
nrow(Data)
Data_miss[which(Data_miss[,i] > lower_bound &
Data_miss[,i] < upper_bound),]
# Eliminating outliers
for (i in Num) {
# Identify outliers using IQR method
Q1 = quantile(Data_miss[,i], 0.25)
Q3 = quantile(Data_miss[,i], 0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
#filtering the Dataset
Data_miss_out = Data_miss[which(Data_miss[,i] > lower_bound &
Data_miss[,i] < upper_bound),]
}
nrow(Data_miss_out)
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num]))
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num]),
method = "pie")
gg_miss_case(Data)
# Correlation matrix
corrplot::corrplot(M, p.mat = testRes$p, method = 'color', diag = FALSE,
type = 'upper', sig.level = c(0.001, 0.01, 0.05),
pch.cex = 0.9, insig = 'label_sig', pch.col = 'grey20',
order = 'AOE')
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num]), p.mat = testRes$p,
method = 'color', diag = FALSE,
type = 'upper', sig.level = c(0.001, 0.01, 0.05),
pch.cex = 0.9, insig = 'label_sig', pch.col = 'grey20',
order = 'AOE')
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num]),
method = 'color', diag = FALSE,
type = 'upper', sig.level = c(0.001, 0.01, 0.05),
pch.cex = 0.9, insig = 'label_sig', pch.col = 'grey20',
order = 'AOE')
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num[2:6]]),
method = 'color', diag = FALSE,
type = 'upper', sig.level = c(0.001, 0.01, 0.05),
pch.cex = 0.9, insig = 'label_sig', pch.col = 'grey20',
order = 'AOE')
?corrplot::corrplot
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', addCoef.col = 'black',
tl.pos = 'd', cl.pos = 'n', col = COL2('PiYG'))
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE')
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', diag = F)
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', diag = F, type = "lower")
# Correlation matrix
corrplot::corrplot(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', diag = F, type = "upper")
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', diag = F, type = "upper")
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE')
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', diag = F)
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE')
corrplot::cor.mtest(cor(Data_miss_out[Num[2:6]]))
?corrplot::corrplot.mixed
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', plotCI = "square")
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', plotCI = "square")
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', upper = "square")
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', upper = "square", diag = "l")
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', upper = "square", diag = "u")
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', upper = "square", diag = "n")
# Correlation matrix
corrplot::corrplot.mixed(corr = cor(Data_miss_out[Num[2:6]]),
order = 'AOE', upper = "square", diag = "n",
addgrid.col = "black")
install.packages("GGally")
# Correlation matrix plot
GGally::ggally_cor(Data_miss_out[Num])
# Correlation matrix plot
GGally::ggally_cor(Data_miss_out[Num[2:6]])
# Correlation matrix plot
GGally::ggpairs(Data_miss_out[Num[2:6]])
GGally::ggally_na(Data)
library(GGally)
knitr::opts_chunk$set(echo=FALSE, eval=TRUE, results='asis', warning=FALSE, message=FALSE, fig.width=5, fig.height=4, fig.align='center', error = FALSE)
library(readr)#Loading data into R
library(summarytools)#summary of the data
library(tidyverse)#Data manipulation
library(flextable)#Table output
library(VIM)#Visualizing missing values
library(naniar)#For handling missing data
library(mice)#Multiple imputation using chained equation
library(kableExtra)#Table
library(ggplot2)#Data visualization
library(GGally)
Data = read.csv("train.csv", stringsAsFactors = T)
# First six rows in the data frame
head(Data)
# summary of the data
dfSummary(Data, graph.col = F)
# Checking for duplicates in the key variable
anyDuplicated(Data$Id)
#Is there any missing values
any(is.na(Data))
#Proportion of missing values per variable
aggr(Data[, colSums(is.na(Data))>0], plot = T)
#80% of the rows
miss = 0.8 * nrow(Data)
#Eliminating variables with more than 80% missing variables
Data = Data[, !colSums(is.na(Data))>miss]
#Sum of missing values per variable
colSums(is.na(Data))[colSums(is.na(Data))>0]
# Plotting the missing data pattern
vis_miss(Data)
# Applying Little's MCAR test
# The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value
#Numerical variables
Num = Data %>% select(where(is.numeric)) %>%
colnames()
#mcar_test(Data[Num])#numerical variables
#omitting all rows with missing values
Data_miss = na.omit(Data)
# Eliminating outliers
for (i in Num) {
# Identify outliers using IQR method
Q1 = quantile(Data_miss[,i], 0.25)
Q3 = quantile(Data_miss[,i], 0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
#filtering the Dataset
Data_miss_out = Data_miss[which(Data_miss[,i] > lower_bound &
Data_miss[,i] < upper_bound),]
}
# Test for normality
ad.test(Data_miss_out$SalePrice)
library(nortest)
# Test for normality
ad.test(Data_miss_out$SalePrice)
unique(Data$SaleCondition)
library(car)
# Levene test
# Null hypothesis variance is Constant
leveneTest(SalePrice~SaleCondition, data = Data)
# Null hypothesis variable is independent
# T test
t.test(Data$SalePrice)
unique(Data$OverallQual)
summary(Data)
# Levene test
# Null hypothesis variance is Constant
leveneTest(SalePrice~SaleCondition, data = Data_miss_out)
# Null hypothesis variable is independent
# T test
t.test(Data_miss_out$LotArea,Data_miss_out$SalePrice)
# Correlation test
cor.test(Data_miss_out$LotArea,Data_miss_out$SalePrice)
# Correlation matrix plot
ggpairs(Data_miss_out[Num[2:6]])
wilcox.test(Data_miss_out$LotArea,Data_miss_out$SalePrice)
?kruskal.test()
?kruskal.test(SalePrice~SaleCondition, data = Data_miss_out)
kruskal.test(SalePrice~SaleCondition, data = Data_miss_out)
# Anova
aov(SalePrice~SaleCondition, data = Data_miss_out)
# Anova
aov(SalePrice~SaleCondition, data = Data_miss_out) %>% summary()
?cor.test()
# Correlation test
cor.test(Data_miss_out$LotArea,Data_miss_out$SalePrice, method = "spearman")
